{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92b69f45",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, SpectralClustering, BisectingKMeans, AgglomerativeClustering, FeatureAgglomeration\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "from embedding_functions_hugo.embedding_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25e8276e",
   "metadata": {},
   "source": [
    "# Data Grabbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45ae085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subreddit dataframes with comments\n",
    "df_gaming = pd.read_csv('../data/scrapes/gaming.csv')\n",
    "df_satis = pd.read_csv('../data/scrapes/SatisfactoryGame.csv')\n",
    "df_marauders = pd.read_csv('../data/scrapes/MaraudersGame.csv')\n",
    "df_tarkov = pd.read_csv('../data/scrapes/EscapefromTarkov.csv')\n",
    "df_politics = pd.read_csv('../data/scrapes/politics.csv')\n",
    "df_antiwork = pd.read_csv('../data/date_folders/april_17/scrapes/antiwork.csv')\n",
    "\n",
    "# loading embeddings\n",
    "politics_embeddings = np.load('../data/embeddings/politics_embeddings.npy')\n",
    "gaming_embeddings = np.load('../data/embeddings/gaming_embeddings.npy')\n",
    "marauders_embeddings = np.load('../data/embeddings/marauders_embeddings.npy')\n",
    "tarkov_embeddings = np.load('../data/embeddings/tarkov_embeddings.npy')\n",
    "satisfactory_embeddings = np.load('../data/embeddings/satisfactory_embeddings.npy')\n",
    "\n",
    "# loading large embeddings\n",
    "politics_embeddings_large = np.load('../data/big_embeddings/politics.npy')\n",
    "gaming_embeddings_large = np.load('../data/big_embeddings/gaming.npy')\n",
    "tarkov_embeddings_large = np.load('../data/big_embeddings/Tarkov.npy')\n",
    "marauders_embeddings_large = np.load('../data/big_embeddings/Marauders.npy')\n",
    "satisfactory_embeddings_large = np.load('../data/big_embeddings/Satisfactory.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d4eb0cf",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c79414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shortening and cleaning function\n",
    "def shorten_and_clean_dataset (comment_csv, comment_column:str, desired_comment_length:int):\n",
    "    dataframe = pd.read_csv(comment_csv)\n",
    "    dataframe['cleaned_text'] = prep_pipeline(dataframe, comment_column)\n",
    "    dataframe['short'] = shorten_sens(dataframe['cleaned_text'], desired_comment_length)\n",
    "    return dataframe\n",
    "\n",
    "# function for creating and saving embeddings\n",
    "def save_embeddings_as_npy(destination_path:str, comment_csv, comment_column:str, desired_comment_length:int):\n",
    "    '''\n",
    "    Nlp pipeline function which takes a pandas dataframe and relevant columns, performs preprocessing steps, uses sentence_transformer embeddings and saves the embeddings as a csv file.\n",
    "    '''\n",
    "    sentences = shorten_and_clean_dataset(comment_csv, comment_column, desired_comment_length)\n",
    "    embeddings = embed_comments(sentences['short'])\n",
    "    return np.save(destination_path, embeddings)\n",
    "\n",
    "def pair_users_embeddings(dataframe, embeddings, average_out_comments = False):\n",
    "    usernames = dataframe['comment_author']\n",
    "    user_dictionary = {}\n",
    "    for author, embedded_comment in zip(usernames, embeddings):\n",
    "        if author not in user_dictionary.keys():\n",
    "            user_dictionary[author] = []\n",
    "            user_dictionary[author].append(embedded_comment)\n",
    "        else:\n",
    "            user_dictionary[author].append(embedded_comment)\n",
    "    if average_out_comments:\n",
    "        for user in user_dictionary:\n",
    "            number_or_comments = len(user_dictionary[user])\n",
    "            user_dictionary[user] = sum(user_dictionary[user])/number_or_comments\n",
    "    return user_dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15a85c92",
   "metadata": {},
   "source": [
    "# Other prep\n",
    "\n",
    "## Adding Cleaned Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_politics['cleaned_text'] = prep_pipeline(df_politics, 'comment_text', loud=False)\n",
    "df_politics['short'] = shorten_sens(df_politics['cleaned_text'], 50)\n",
    "\n",
    "df_gaming['cleaned_text'] = prep_pipeline(df_gaming, 'comment_text', loud=False)\n",
    "df_gaming['short'] = shorten_sens(df_gaming['cleaned_text'], 50)\n",
    "\n",
    "df_tarkov['cleaned_text'] = prep_pipeline(df_tarkov, 'comment_text', loud=False)\n",
    "df_tarkov['short'] = shorten_sens(df_tarkov['cleaned_text'], 50)\n",
    "\n",
    "df_marauders['cleaned_text'] = prep_pipeline(df_marauders, 'comment_text', loud=False)\n",
    "df_marauders['short'] = shorten_sens(df_marauders['cleaned_text'], 50)\n",
    "\n",
    "df_satis['cleaned_text'] = prep_pipeline(df_satis, 'comment_text', loud=False)\n",
    "df_satis['short'] = shorten_sens(df_satis['cleaned_text'], 50)\n",
    "\n",
    "df_antiwork['cleaned_text'] = prep_pipeline(df_antiwork, 'comment_text', loud=False)\n",
    "df_antiwork['short'] = shorten_sens(df_antiwork['cleaned_text'], 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7e48f83b",
   "metadata": {},
   "source": [
    "# Specific inspection\n",
    "\n",
    "### Antiwork example where user posts something with broad negativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f23b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "antiwork_embeddings = save_embeddings_as_npy('../data/embeddings/',\n",
    "                                             '../data/date_folders/april_17/scrapes/antiwork.csv',\n",
    "                                             'comment_text',\n",
    "                                             50)\n",
    "antiwork_user_embeddings = pair_users_embeddings(df_antiwork, antiwork_embeddings, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ea8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_antiwork.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e8c889",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_antiwork['post_title'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadc5558",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_antiwork.iterrows():\n",
    "    if row['post_title'] == 'Republicans are not pro life. And they need to be stopped immediately. They’re anti humanity':\n",
    "        # print(row['post_title'])\n",
    "        print(row['post_url'])\n",
    "        # if row['comment_author'] == 'Southern_Nature_5416':\n",
    "        print('========================================================')\n",
    "        # print(row['post_title'])\n",
    "        print(row['comment_author'])\n",
    "        print(row['text_clean'])\n",
    "\n",
    "        from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "        def sentiment_vader(sentence):\n",
    "            # Create a SentimentIntensityAnalyzer object.\n",
    "            sid_obj = SentimentIntensityAnalyzer()\n",
    "\n",
    "            sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "            compound = sentiment_dict['compound']\n",
    "            \n",
    "            return compound\n",
    "        print(sentiment_vader(row['text_clean']))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da5d5c96",
   "metadata": {},
   "source": [
    "### Strong Antiwork Post\n",
    "\"Republicans are not pro life. And they need to be stopped immediately. They’re anti humanity\"\n",
    "\n",
    "### Strong comment left on post (by LadyMageCOH, sentiment -0.9921):\n",
    "\n",
    "republicans never have been prolife  one of the carolinas is trying to pass or may have already passed a bill that classes abortion as felony murder  felony murder in the same state is potentially punishable by the death penalty  \n",
    "\n",
    "so if a woman is raped ends up pregnant and aborts the pregnancy could end up getting more time than her rapist and possibly get executed  \n",
    "\n",
    "even more appalling  many of these abortion bans that have come into effect since the dobbs decision are worded so poorly that they could get a woman arrested for a spontaneous abortion aka miscarriage\n",
    "\n",
    "### Shorter strong comment left on post (by Southern_Nature_5416, sentiment -0.7783)\n",
    "\n",
    "texas wants to be like australia but instead of animals trying to kill you it is republicans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "111e0f19",
   "metadata": {},
   "source": [
    "# Comment Inspection\n",
    "\n",
    "## Quick Prep (r/politics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed691c",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_user_embeddings = pair_users_embeddings(df_politics, politics_embeddings, True)\n",
    "\n",
    "# Set PCA to desired number of dimensions\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "pca_embeddings = pca.fit_transform(list(politics_user_embeddings.values()))\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "\n",
    "classes = kmeans.fit_predict(pca_embeddings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a7e0a2a",
   "metadata": {},
   "source": [
    "## Horizontally distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164b49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_embeddings.shape)\n",
    "#print(len(politics_user_embeddings.keys()), len(politics_user_embeddings.values()))\n",
    "\n",
    "# finding indexes of rows with least and max x values\n",
    "\n",
    "x_vals = []\n",
    "for idx, row in enumerate(pca_embeddings):\n",
    "    x_val = row[0]\n",
    "    x_vals.append(x_val)\n",
    "\n",
    "# least x\n",
    "least_x = min(x_vals)\n",
    "least_x_index = np.argmin(x_vals)\n",
    "least_x_username = list(politics_user_embeddings.keys())[least_x_index]\n",
    "least_x_comments = df_politics.loc[df_politics['comment_author'] == least_x_username]\n",
    "\n",
    "max_x = max(x_vals)\n",
    "max_x_index = np.argmax(x_vals)\n",
    "max_x_username = list(politics_user_embeddings.keys())[max_x_index]\n",
    "max_x_comments = df_politics.loc[df_politics['comment_author'] == max_x_username]\n",
    "\n",
    "#print(least_x, least_x_index, least_x_username)\n",
    "#print(max_x, max_x_index, max_x_username)\n",
    "\n",
    "#print(df_politics.shape)\n",
    "#print(len(politics_user_embeddings.keys()))\n",
    "print('===== Lowest x coord comment =====')\n",
    "print(least_x_comments['comment_text'].values[0])\n",
    "print('\\n===== Highest x coord comment =====')\n",
    "print(max_x_comments['comment_text'].values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d8fbc11a",
   "metadata": {},
   "source": [
    "## Vertically distant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d225e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pca_embeddings.shape)\n",
    "#print(len(politics_user_embeddings.keys()), len(politics_user_embeddings.values()))\n",
    "\n",
    "# finding indexes of rows with least and max y values\n",
    "\n",
    "y_vals = []\n",
    "for idx, row in enumerate(pca_embeddings):\n",
    "    y_val = row[1]\n",
    "    y_vals.append(y_val)\n",
    "\n",
    "# least y\n",
    "least_y = min(y_vals)\n",
    "least_y_index = np.argmin(y_vals)\n",
    "least_y_username = list(politics_user_embeddings.keys())[least_y_index]\n",
    "least_y_comments = df_politics.loc[df_politics['comment_author'] == least_y_username]\n",
    "\n",
    "max_y = max(y_vals)\n",
    "max_y_index = np.argmax(y_vals)\n",
    "max_y_username = list(politics_user_embeddings.keys())[max_y_index]\n",
    "max_y_comments = df_politics.loc[df_politics['comment_author'] == max_y_username]\n",
    "\n",
    "#print(least_y, least_y_index, least_y_username)\n",
    "#print(max_y, max_y_index, max_y_username)\n",
    "\n",
    "#print(df_politics.shape)\n",
    "#print(len(politics_user_embeddings.keys()))\n",
    "print('===== Lowest y coord comment =====')\n",
    "print(least_y_comments['comment_text'].values[0])\n",
    "print()\n",
    "print('===== Highest y coord comment =====')\n",
    "print(max_y_comments['comment_text'].values[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f121e39",
   "metadata": {},
   "source": [
    "## Most distant (in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324c0c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_distant(df, embeddings, n=3, large_embeds=True):\n",
    "    '''\n",
    "    inputs:\n",
    "     - df: df to work with\n",
    "     - embeddings: embeddings to work with\n",
    "     - n: number of most distant user-pairs to print\n",
    "    \n",
    "    Function calculates the distance between all user pairs and sorts them from most to least distant.\n",
    "    n number of user-pairs have their comments printed\n",
    "    '''\n",
    "    \n",
    "    ### TODO DONT USE pair_users_embeddings\n",
    "    # # pairing embeddings\n",
    "    # user_embeddings = pair_users_embeddings(df, embeddings, False)\n",
    "    \n",
    "    # # doing pca things\n",
    "    # pca = PCA(n_components=2)\n",
    "    # pca_embeddings = pca.fit_transform(list(user_embeddings.values()))\n",
    "\n",
    "    # looping through dataframe and embeddings to get username list that matches index-wise\n",
    "    # for embeddings that include every comment (duplicate authors):\n",
    "    if large_embeds == False:\n",
    "        username_list = list(df['comment_author'])\n",
    "    # for embeddings that have averaged authors (no duplicate authors):\n",
    "    else:\n",
    "        username_list = list()\n",
    "        for index, row in df.iterrows():\n",
    "            username = row['comment_author']\n",
    "            if username not in username_list:\n",
    "                username_list.append(username)\n",
    "    \n",
    "    # getting user pairs\n",
    "    pair_dict = dict()\n",
    "    for i, j in zip(username_list, embeddings):\n",
    "        pair_dict[i] = j\n",
    "    \n",
    "    # reducing embeddings to 2 dimensions\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(list(pair_dict.values()))\n",
    "\n",
    "    # classifying using kmeans and printing blob\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "    classes = kmeans.fit_predict(pca_embeddings)\n",
    "    label_color_map = {0 : 'r',1 : 'g'}\n",
    "    label_color = [label_color_map[l] for l in classes]\n",
    "    plt.scatter(pca_embeddings[:,0], pca_embeddings[:,1], c=label_color)\n",
    "    \n",
    "    distance_list = list() # list for keeping track of maximum distances, index matches the user the distance is from\n",
    "    distance_to_whom_list = list() # list for keeping track of the index of the user the distance is to\n",
    "    \n",
    "    num_rows = pca_embeddings.shape[0]\n",
    "    print(f'looping through {num_rows} rows, this may take a few mins...')\n",
    "    \n",
    "    # looping through embeddings to loop through users\n",
    "    for idx_a, row_a in enumerate(pca_embeddings):\n",
    "        x_val_a = row_a[0]\n",
    "        y_val_a = row_a[1]\n",
    "        \n",
    "        max_distance = 0.0\n",
    "        to_whom_index = 0\n",
    "        for idx_b, row_b in enumerate(pca_embeddings):\n",
    "            x_val_b = row_b[0]\n",
    "            y_val_b = row_b[1]\n",
    "            \n",
    "            # calculating euclidean distance\n",
    "            a = (x_val_a, y_val_a)\n",
    "            b = (x_val_b, y_val_b)\n",
    "            dist = distance.euclidean(a, b)\n",
    "            \n",
    "            # saving to distance_list and distance_to_whom_list\n",
    "            if dist > max_distance:\n",
    "                max_distance = dist\n",
    "                to_whom_index = idx_b\n",
    "        \n",
    "        distance_list.append(max_distance)\n",
    "        distance_to_whom_list.append(to_whom_index)\n",
    "    \n",
    "    # sorting distance_list\n",
    "    top_n_distances = np.sort(distance_list)\n",
    "    top_n_indexes = np.argsort(distance_list)[-n:]\n",
    "    \n",
    "    #print(top_n_distances)\n",
    "    #print(top_n_indexes)\n",
    "    \n",
    "    n_pairs = dict()\n",
    "    for a_idx in top_n_indexes:\n",
    "        n_pairs[a_idx] = [distance_to_whom_list[a_idx], top_n_distances[a_idx]]\n",
    "    \n",
    "    #print(n_pairs)\n",
    "    \n",
    "    # gettings comments of pairs and printing them\n",
    "    usernames_a = list()\n",
    "    for index in n_pairs.keys():\n",
    "        username = username_list[index]\n",
    "        usernames_a.append(username)\n",
    "    \n",
    "    usernames_b = list()\n",
    "    for index in list(n_pairs.values()):\n",
    "        username = username_list[index[0]]\n",
    "        usernames_b.append(username)\n",
    "    \n",
    "    for pair in zip(usernames_a, usernames_b):\n",
    "        username_a = pair[0]\n",
    "        username_b = pair[1]\n",
    "        \n",
    "        comments_a = df.loc[df['comment_author'] == username_a]\n",
    "        comments_b = df.loc[df['comment_author'] == username_b]\n",
    "        \n",
    "        print(f'\\n===== Pair between {username_a} and {username_b} =====')\n",
    "        print(f' == {username_a}:\\n{comments_a[\"short\"].values[0]}')\n",
    "        print(f' == {username_b}:\\n{comments_b[\"short\"].values[0]}')\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3c2f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_satis.loc[df_satis['comment_author'] == 'Hob_O_Rarison']['comment_text'].values[0])\n",
    "print(df_satis.loc[df_satis['comment_author'] == 'Hob_O_Rarison']['short'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8117f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairing embeddings\n",
    "user_embeddings = pair_users_embeddings(df_satis, satisfactory_embeddings, True)\n",
    "\n",
    "# doing pca things\n",
    "pca = PCA(n_components=2)\n",
    "pca_embeddings = pca.fit_transform(list(user_embeddings.values()))\n",
    "kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "classes = kmeans.fit_predict(pca_embeddings)\n",
    "\n",
    "print(pca_embeddings[456])\n",
    "print(pca_embeddings[1435])\n",
    "print(pca_embeddings[1425])\n",
    "print(pca_embeddings[2599])\n",
    "print(pca_embeddings[2322])\n",
    "print(pca_embeddings[2429])\n",
    "\n",
    "print(list(user_embeddings.keys())[1435])\n",
    "print(df_satis.loc[df_satis['comment_author'] == list(user_embeddings.keys())[1435]]['short'].values[0])\n",
    "print(list(user_embeddings.keys())[1425])\n",
    "print(df_satis.loc[df_satis['comment_author'] == list(user_embeddings.keys())[1425]]['short'].values[0])\n",
    "print(list(user_embeddings.keys())[2599])\n",
    "print(df_satis.loc[df_satis['comment_author'] == list(user_embeddings.keys())[2599]]['short'].values[0])\n",
    "print(list(user_embeddings.keys())[2322])\n",
    "print(df_satis.loc[df_satis['comment_author'] == list(user_embeddings.keys())[2322]]['short'].values[0])\n",
    "print(list(user_embeddings.keys())[2429])\n",
    "print(df_satis.loc[df_satis['comment_author'] == list(user_embeddings.keys())[2429]]['short'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88e2956",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "most_distant(df_satis, satisfactory_embeddings_large, n=20, large_embeds=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd8e5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_distant(df_politics, politics_embeddings_large, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d11e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_distant(df_gaming, gaming_embeddings_large, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7dd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_distant(df_tarkov, tarkov_embeddings_large, n=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1999d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_distant(df_marauders, marauders_embeddings_large, n=20)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b04590f5",
   "metadata": {},
   "source": [
    "## Similar Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ba13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(df, embeddings):\n",
    "    '''\n",
    "    inputs:\n",
    "     - df: df to work with\n",
    "     - embeddings: embeddings to work with\n",
    "\n",
    "    function finds similar groups of users in different areas of the embeddings space and prints one of their comments:\n",
    "     - MIDDLE: All users around 0 (+/-0.1) are grouped, the amount of users here can vary\n",
    "     - LEFT, RIGHT, TOP, BOTTOM: For each of these sides 5 users that are the most of them are grouped\n",
    "    '''\n",
    "    \n",
    "    # pairing embeddings\n",
    "    user_embeddings = pair_users_embeddings(df, embeddings, True)\n",
    "    \n",
    "    # doing pca things\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_embeddings = pca.fit_transform(list(user_embeddings.values()))\n",
    "    kmeans = KMeans(n_clusters=2, random_state=0)\n",
    "    classes = kmeans.fit_predict(pca_embeddings)\n",
    "    \n",
    "    # print blob\n",
    "    label_color_map = {0 : 'r',1 : 'g'}\n",
    "    label_color = [label_color_map[l] for l in classes]\n",
    "    plt.scatter(pca_embeddings[:,0], pca_embeddings[:,1], c=label_color)\n",
    "    \n",
    "    # finding similar things\n",
    "    # NEED TO CODE FOR FINDING ALL FAR THINGS\n",
    "    to_check = ['MIDDLE', 'LEFT', 'RIGHT', 'TOP', 'BOTTOM']\n",
    "    \n",
    "    for i in to_check:\n",
    "        \n",
    "        # finding x and y limits based off of blob\n",
    "        \n",
    "        if i == 'MIDDLE':\n",
    "            print('========== MIDDLE ==========')\n",
    "            x_lims = [-0.1, 0.1]\n",
    "            y_lims = [-0.1, 0.1]\n",
    "            \n",
    "            similar_indexes = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                x_val = row[0]\n",
    "                y_val = row[1]\n",
    "                \n",
    "                if x_val > x_lims[0] and x_val < x_lims[1] and y_val > y_lims[0] and y_val < y_lims[1]:\n",
    "                    similar_indexes.append(idx)\n",
    "        \n",
    "        # checks from far left and finds first 5 comments\n",
    "        elif i == 'LEFT':\n",
    "            print('========== LEFT ==========')\n",
    "            # get list of x coords for sorting\n",
    "            x_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                x_coords.append(row[0])\n",
    "            \n",
    "            # sorts x coords by ascending, but gives the indexes not the values\n",
    "            sorted_indexes = np.argsort(x_coords)\n",
    "            \n",
    "            similar_indexes = sorted_indexes[:5]\n",
    "        \n",
    "        # checks from far left and finds first 5 comments\n",
    "        elif i == 'RIGHT':\n",
    "            print('========== RIGHT ==========')\n",
    "            # get list of x coords for sorting\n",
    "            x_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                x_coords.append(row[0])\n",
    "            \n",
    "            # sorts x coords by descending, but gives the indexes not the values\n",
    "            initial_sort = np.argsort(x_coords)\n",
    "            \n",
    "            similar_indexes = initial_sort[::-1][:5] # 5 for first 5 comments\n",
    "        \n",
    "        # checks from far top and finds first 5 comments\n",
    "        elif i == 'TOP':\n",
    "            print('========== TOP ==========')\n",
    "            # get list of y coords for sorting\n",
    "            y_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                y_coords.append(row[1])\n",
    "            \n",
    "            # sorts y coords by descending, but gives the indexes not the values\n",
    "            initial_sort = np.argsort(y_coords)\n",
    "            \n",
    "            similar_indexes = initial_sort[::-1][:5] # 5 for first 5 comments\n",
    "        \n",
    "        elif i == 'BOTTOM':\n",
    "            print('========== BOTTOM ==========')\n",
    "            # get list of y coords for sorting\n",
    "            y_coords = list()\n",
    "            for idx, row in enumerate(pca_embeddings):\n",
    "                y_coords.append(row[1])\n",
    "            \n",
    "            # sorts y coords by ascending, but gives the indexes not the values\n",
    "            sorted_indexes = np.argsort(y_coords)\n",
    "            \n",
    "            similar_indexes = sorted_indexes[:5]\n",
    "            \n",
    "        # using list of similar indexes, matches with users and prints their comments\n",
    "        usernames = list()\n",
    "        for index in similar_indexes:\n",
    "            username = list(user_embeddings.keys())[index]\n",
    "            usernames.append(username)\n",
    "\n",
    "        # cleaning comments to get relevant ones in embedding space\n",
    "        df['cleaned_text'] = prep_pipeline(df, 'comment_text', loud=False)\n",
    "        df['short'] = shorten_sens(df['cleaned_text'], 50)\n",
    "        \n",
    "        for username in usernames:\n",
    "            comments = df.loc[df['comment_author'] == username]\n",
    "            #print(comments['comment_text'].values[0], '\\n')\n",
    "            print(f'{username}:')\n",
    "            print(comments['short'].values[0], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfd9da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_politics, politics_embeddings_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e0f100",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_gaming, gaming_embeddings_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d75787",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_marauders, marauders_embeddings_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8acc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_tarkov, tarkov_embeddings_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf9289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(df_satis, satisfactory_embeddings_large)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d3965c0036c6634b7a8209547ff43f0035ae86a5cd15445377c7a137a4d311"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
