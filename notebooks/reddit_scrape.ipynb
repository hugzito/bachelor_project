{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Tutorial for reddit scraping: https://www.geeksforgeeks.org/scraping-reddit-using-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "from praw.models import MoreComments\n",
    "# from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ids for scraping (from christians setup)\n",
    "client_id = 'Ut5UgaAMOEWBELtYRWnw0g'\n",
    "client_secret = '5xGs1w6mav5Ke685afpG28Q8nfusmg'\n",
    "user_agent = 'polarity search'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping\n",
    "\n",
    "First we initialize a read-only instance. A read-only instance can only scrape publicly available information and cannot upvote or otherwise interact like users can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read-only instance\n",
    "reddit_read_only = praw.Reddit(client_id=client_id,         # your client id\n",
    "                               client_secret=client_secret,      # your client secret\n",
    "                               user_agent=user_agent)        # your user agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on a specific post\n",
    "\n",
    "This code scrapes over the comments of a specified post. It looks only at the top-level comments (none of the replies to comments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_post(url, n=100):\n",
    "    '''given a url:\n",
    "     - scrapes n number of top-level comments\n",
    "     - comment author (username) and comment body ()\n",
    "     - outputs a pandas dataframe'''\n",
    "\n",
    "    # Creating a submission object\n",
    "    submission = reddit_read_only.submission(url=url)\n",
    "    \n",
    "    # should get all top level comments on the post\n",
    "    #if all_comments==True:\n",
    "    #    submission.comments.replace_more(limit=None)\n",
    "\n",
    "    post_authors = []\n",
    "    post_comments = []\n",
    "\n",
    "    # specifying how many times we should \"load more comments\"\n",
    "    limit = round(n/20) # n/20 ensures we hit load more enough times to scrape the number of comments we want \n",
    "    submission.comments.replace_more(limit=limit, threshold=0)\n",
    "\n",
    "    count = 0 # tracking how many comments have been scraped\n",
    "    for comment in submission.comments: # iterates only over top level comments\n",
    "        # stops counting when n amount of comments have been scraped\n",
    "        if count == n:\n",
    "            break\n",
    "\n",
    "        if type(comment) == MoreComments:\n",
    "            continue\n",
    "\n",
    "        post_authors.append(comment.author)\n",
    "        post_comments.append(comment.body)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    post_dict = {'author': post_authors, 'comment': post_comments}\n",
    "    post_df = pd.DataFrame(post_dict)\n",
    "    \n",
    "    return post_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of how scraping a post works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lozsta</td>\n",
       "      <td>Why is there not a toggle to turn that off. I ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OpossumHades</td>\n",
       "      <td>...that destroyed ÖRTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JEClockwork</td>\n",
       "      <td>For 70 years we have long lived in the shadows...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>l3lNova</td>\n",
       "      <td>Ok but real talk that movie was wack</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sw4mpy_1</td>\n",
       "      <td>Well no more!!!!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         author                                            comment\n",
       "0        Lozsta  Why is there not a toggle to turn that off. I ...\n",
       "1  OpossumHades                             ...that destroyed ÖRTH\n",
       "2   JEClockwork  For 70 years we have long lived in the shadows...\n",
       "3       l3lNova               Ok but real talk that movie was wack\n",
       "4      sw4mpy_1                                   Well no more!!!!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scraping a post without much content - takes <1 second\n",
    "# there are 12 top level comments\n",
    "df = scrape_post(\"https://www.reddit.com/r/MaraudersGame/comments/ylxsq4/marauders_be_like/\")\n",
    "\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping a decently sized post (function default scrapes 100 comments) - takes ~10 seconds\n",
    "# df = scrape_post(\"https://www.reddit.com/r/politics/comments/1092xhl/the_american_public_no_longer_believes_the/\")\n",
    "\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # scraping 200 comments from a decently sized post (post has 3.8k comments) - takes ~15 seconds\n",
    "# df = scrape_post(\"https://www.reddit.com/r/politics/comments/1092xhl/the_american_public_no_longer_believes_the/\", n=200)\n",
    "\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aidan: idk what below line was so i just commented it out\n",
    "# praw.models.reddit.submission.Submission?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting top month posts on specified subreddit\n",
    "This code grabs the top 100 posts of the past month and saves various information on them into a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_top_month(subreddit, ppsr=100):\n",
    "    # specifying subreddit\n",
    "    subreddit = reddit_read_only.subreddit(subreddit)\n",
    "\n",
    "    # Specifying to look at top posts of the current month\n",
    "    posts = subreddit.top(\"month\", limit=ppsr)\n",
    "\n",
    "    # Initializing dictionary to save post data to\n",
    "    posts_dict = {\"Title\": [], \"Post Text\": [],\n",
    "                  \"ID\": [], \"Score\": [],\n",
    "                  \"Total Comments\": [], \"Post URL\": [], 'Post_author' : []\n",
    "                  }\n",
    "\n",
    "    # Loop for saving post details\n",
    "    for post in posts:\n",
    "        # print(post)\n",
    "        # Title of each post\n",
    "        posts_dict[\"Title\"].append(post.title)\n",
    "\n",
    "        # Text inside a post\n",
    "        posts_dict[\"Post Text\"].append(post.selftext)\n",
    "\n",
    "        # Unique ID of each post\n",
    "        posts_dict[\"ID\"].append(post.id)\n",
    "\n",
    "        # The score of a post\n",
    "        posts_dict[\"Score\"].append(post.score)\n",
    "\n",
    "        # Total number of comments inside the post\n",
    "        posts_dict[\"Total Comments\"].append(post.num_comments)\n",
    "\n",
    "        # Author of the post\n",
    "        posts_dict['Post_author'].append(post.author)\n",
    "\n",
    "        # URL of each post\n",
    "        # print('https://www.reddit.com'+f'{post.permalink}')\n",
    "        posts_dict[\"Post URL\"].append('https://www.reddit.com'+f'{post.permalink}')\n",
    "        \n",
    "    return posts_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples of how scraping top month posts works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/1v9m4ykn66ddbcxt5nwgg9280000gn/T/ipykernel_49201/1251007598.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\", limit=ppsr)\n"
     ]
    }
   ],
   "source": [
    "dict_ = scrape_top_month('politics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ocasio-Cortez calls for Thomas impeachment after report of undisclosed gifts from GOP donor\n",
      "\n",
      "12dna0j\n",
      "103880\n",
      "3411\n",
      "https://www.reddit.com/r/politics/comments/12dna0j/ocasiocortez_calls_for_thomas_impeachment_after/\n",
      "100\n",
      "Gato1980\n"
     ]
    }
   ],
   "source": [
    "# post samples\n",
    "print(dict_['Title'][0])\n",
    "print(dict_['Post Text'][0])\n",
    "print(dict_['ID'][0])\n",
    "print(dict_['Score'][0])\n",
    "print(dict_['Total Comments'][0])\n",
    "print(dict_['Post URL'][0])\n",
    "print(len(dict_['Title']))\n",
    "print(dict_['Post_author'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_ = scrape_top_month('politics', ppsr=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # post samples\n",
    "# print(dict_['Title'][0])\n",
    "# print(dict_['Post Text'][0])\n",
    "# print(dict_['ID'][0])\n",
    "# print(dict_['Score'][0])\n",
    "# print(dict_['Total Comments'][0])\n",
    "# print(dict_['Post URL'][0])\n",
    "# print(len(dict_['Title']))\n",
    "# print(dict_['Post_author'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting comments on top monthly posts on multiple subreddits\n",
    "\n",
    "Data we are keeping and why:\n",
    " - post_title: to embed and become node values of our users\n",
    " - post_url: in case we ever want to visit the post for any inspection reasons\n",
    " - comment_author: to keep track of who left the comment\n",
    " - comment_text: to analyse the sentiment of the comment towards the post\n",
    " - post_author: to track who made the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_save(subreddits, ppsr=100, n=100, save=False, destination=''):\n",
    "    '''scrapes and saves subreddit comments to csv files\n",
    "     - subreddits: list of subreddit name strings, fx Politics\n",
    "     - ppsr: number of posts per subbreddit to scrape\n",
    "     - n: number of comments per post to scrape'''\n",
    "    \n",
    "    if save==False:\n",
    "        print('WARNING: save=True has not been specified!')\n",
    "\n",
    "    print(f'Scraping {ppsr} posts per subreddit and {n} comments per post')\n",
    "    \n",
    "    # looping through subreddits\n",
    "    for subreddit in subreddits:\n",
    "        print(f'Scraping r/{subreddit}...')\n",
    "        \n",
    "        # initialize dictionary for saving all comments and post info\n",
    "        sub_dict = {'post_title': [],\n",
    "                    # 'post_text': [],\n",
    "                    # 'post_id': [],\n",
    "                    # 'post_score': [],\n",
    "                    # 'post_total_comments': [],\n",
    "                    'post_url': [],\n",
    "                    'comment_author': [],\n",
    "                    'comment_text': [], \n",
    "                    'post_author' : []}\n",
    "        \n",
    "        posts_dict = scrape_top_month(subreddit, ppsr) # getting top of the month post info\n",
    "        \n",
    "        # looping through posts\n",
    "        for idx, url in tqdm(enumerate(posts_dict['Post URL']),):\n",
    "            \n",
    "            # df for comments on the post\n",
    "            comment_df = scrape_post(url, n=n)\n",
    "            \n",
    "            # looping through comments on post and appending all comment info to sub_dict\n",
    "            for row_idx, row in comment_df.iterrows():\n",
    "                sub_dict['post_title'].append(posts_dict['Title'][idx])\n",
    "                # sub_dict['post_text'].append(posts_dict['Post Text'][idx])\n",
    "                # sub_dict['post_id'].append(posts_dict['ID'][idx])\n",
    "                # sub_dict['post_score'].append(posts_dict['Score'][idx])\n",
    "                # sub_dict['post_total_comments'].append(posts_dict['Total Comments'][idx])\n",
    "                sub_dict['post_url'].append(posts_dict['Post URL'][idx])\n",
    "                sub_dict['comment_author'].append(row['author'])\n",
    "                sub_dict['comment_text'].append(row['comment'])\n",
    "                sub_dict['post_author'].append(posts_dict['Post_author'][idx])\n",
    "            \n",
    "        # changing sub_dict to pandas dataframe\n",
    "        global sub_df\n",
    "        sub_df = pd.DataFrame.from_dict(sub_dict)\n",
    "\n",
    "        # saving to csv\n",
    "        if save==True:\n",
    "            sub_df.to_csv(f'{destination}{subreddit}.csv', index=False)\n",
    "        \n",
    "    print('Done!')\n",
    "\n",
    "    if save==True:\n",
    "        return None\n",
    "    else:\n",
    "        return sub_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use function\n",
    "\n",
    "Below line shows how to scrape r/politics, scraping 200 posts and 100 comments from each post.\n",
    "\n",
    "It won't save the dataframe unless save=True is specified, so that it can be tested without overwriting anything.\n",
    "\n",
    "It takes ~ 30-40 minutes to run as it's scraping 200*100 = 20,000 comments. It won't end up actually being 20,000 comments however, probably because 200 posts down there will be less than 100 top-level comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~ 40 minutes to run (it's scraping 200*100 = 20,000 comments - though it wont be that many)\n",
    "#df_test = scrape_multiple_save(['politics'], ppsr=200, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df_test.shape)\n",
    "#df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testy = df_test.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running scrapes - ONLY RUN IF WILLING TO LEAVE RUNNING FOR HOURS\n",
    "\n",
    "Make sure to specify destination folder before running. Example destination: destination='../data/28feb/scrapes/'\n",
    "\n",
    "Also make sure that the folder destination exists before running, I'm not 100% sure if it is necessary, but it's better to be sure that the dataframe is saveable instead of running the code for an hour and getting a \"destination doesn't exist\" error..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~65 mins to run, finds 33k comments\n",
    "#scrape_multiple_save(['politics'], ppsr=200, n=200, save=True, destination='../data/19march/scrapes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_politics = pd.read_csv('../data/19march/scrapes/politics.csv')\n",
    "#df_politics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~38 mins to run, finds 28k comments\n",
    "#scrape_multiple_save(['gaming'], ppsr=200, n=200, save=True, destination='../data/19march/scrapes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_gaming = pd.read_csv('../data/19march/scrapes/gaming.csv')\n",
    "#df_gaming.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~28 mins to run, finds 28k comments\n",
    "#scrape_multiple_save(['EscapefromTarkov'], ppsr=200, n=200, save=True, destination='../data/19march/scrapes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tarkov = pd.read_csv('../data/19march/scrapes/EscapefromTarkov.csv')\n",
    "#df_tarkov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~7 mins to run, ~6k comments\n",
    "#scrape_multiple_save(['HuntShowdown'], ppsr=200, n=200, save=True, destination='../data/19march/scrapes/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_HuntShowdown = pd.read_csv('../data/19march/scrapes/HuntShowdown.csv')\n",
    "#df_HuntShowdown.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping even larger datasets (Gonna leave pc running for a long time)\n",
    "-Chris "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_multiple_save(['politics'], ppsr=1000, n=200, save=True, destination='../data/23march_chur/scrape/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scrape_multiple_save(['gaming'], ppsr=1000, n=200, save=True, destination='../data/23march_chur/scrape/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1000 posts per subreddit and 200 comments per post\n",
      "Scraping r/FIFA...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/1v9m4ykn66ddbcxt5nwgg9280000gn/T/ipykernel_49201/1251007598.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\", limit=ppsr)\n",
      "999it [36:47,  2.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "scrape_multiple_save(['FIFA'], ppsr=1000, n=200, save=True, destination='../data/date_folders/april_18/scrape/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping 1000 posts per subreddit and 200 comments per post\n",
      "Scraping r/CallOfDuty...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7n/1v9m4ykn66ddbcxt5nwgg9280000gn/T/ipykernel_49201/1251007598.py:6: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\", limit=ppsr)\n",
      "397it [09:46,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "scrape_multiple_save(['CallOfDuty'], ppsr=1000, n=200, save=True, destination='../data/date_folders/april_18/scrape/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
